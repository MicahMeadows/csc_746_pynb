{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"unsupervised_learning\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K- Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from input file\n",
    "X = np.loadtxt('lab06_data.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
    "save_fig(\"original_data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kmeans instance preserves a copy of the labels of the instances it was trained on, available via the \"labels_\" instance variable.The five centroids can be accessed with the \"cluster_centers_\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kmeans.labels_:\")\n",
    "print(kmeans.labels_)\n",
    "print(\"kmeans.cluster_centers_:\")\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "# Check the value of y_kmeans here and what observation do you get? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "lb = ['c0', 'c1', 'c2', 'c3', 'c4']\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels = lb, title=\"clusters\")\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='black', s=200, alpha=0.5);\n",
    "save_fig(\"Kmeans_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new instances to the cluster whose centroid is closest:\n",
    "X_new = np.array([[0, 2], [3.5, 6], [5, 9.5], [7, 2.5]])\n",
    "kmeans.predict(X_new)\n",
    "\n",
    "# Check which cluster the following instances belong to?\n",
    "# (-1, 0.5), (6.5, 4.5), (2.2, 8.5) \n",
    "# Sample output:\n",
    "# [0,2] belongs to cluster 4\n",
    "# [3.5, 6] belongs to cluster 0\n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=42):\n",
    "    # Randomly choose the initial centroids\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    \n",
    "    while True:\n",
    "        # Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0) # axis = 0\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 5)\n",
    "\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "lb = ['c0', 'c1', 'c2', 'c3', 'c4']\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels = lb, title=\"clusters\")\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "\n",
    "save_fig(\"user_Kmeans_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the centroids found by the user defined KMeans function here and compare the results to the sklearn method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the algorithm is guaranteed to converge, it may not converge to the right solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # Randomly choose the initial centroids\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    \n",
    "    while True:\n",
    "        # Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0) # axis = 0\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 5)\n",
    "\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "lb = ['c0', 'c1', 'c2', 'c3', 'c4']\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels = lb, title=\"clusters\")\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "save_fig(\"local_optimum_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroid initialization methods\n",
    "1. If you happen to know approximately where the centroids should be (e.g., if you ran another clustering algorithm earlier), then you can set the init hyperparameter to a NumPy array containing the list of centroids, and set n_init to 1:\n",
    "```python\n",
    "good_init = np.array([[5, 5], [2, 8], [7, 2], [6, 9], [3, 2]])\n",
    "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)\n",
    "```\n",
    "2. Another solution is to run the algorithm multiple times with different random initializations and keep the best solution. The number of random initializations is controlled by the n_init hyperparameter: by default, it is equal to 10. The performance can be evaluated by the metric \"inertia\" (cost). Inertia is the sum of the squared distances between each training instance and its closest centroid. The KMeans class runs the algorithm n_init times and keeps the model with the lowest inertia.\n",
    "3. K-Means++ algorithm: It introduced a smarter initialization step that tends to select centroids that are distant from one another, and this improvement makes the K-Means algorithm much less likely to converge to a suboptimal solution. To set the initialization to K-Means++, simply set init=\"k-means++\" (this is actually the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inertia and score values of the above model\n",
    "print(\"inertia:\" +str(kmeans.inertia_))\n",
    "# The score() method returns the negative inertia. \n",
    "# A predictor's score() method must always respect the \"_greater is better_\" rule.\n",
    "print(\"score:\" +str(kmeans.score(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch K-Means\n",
    "Scikit-Learn also implements a variant of the K-Means algorithm that supports mini-batches. Mini-batch K-Means is much faster than regular K-Means. However, its performance is often lower (higher inertia), and it keeps degrading as k increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)\n",
    "minibatch_kmeans.fit(X)\n",
    "minibatch_kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal number of clusters\n",
    "What if the number of clusters was set to a lower or greater value than 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_k3.fit(X)\n",
    "y_kmeans_k3 = kmeans_k3.predict(X)\n",
    "\n",
    "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
    "kmeans_k8.fit(X)\n",
    "y_kmeans_k8 = kmeans_k8.predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans_k3, s=50, cmap='viridis')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans_k8, s=50, cmap='viridis')\n",
    "save_fig(\"bad_n_clusters_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the inertia values\n",
    "print(\"cluster = 3 -- \" + str(kmeans_k3.inertia_))\n",
    "print(\"cluster = 8 -- \" + str(kmeans_k8.inertia_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot simply take the value of K that minimizes the inertia, since it keeps getting lower as we increase K. However, we can plot the inertia as a function of K and analyze the resulting curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The elbow method\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$K$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow',\n",
    "             xy=(5, inertias[4]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "save_fig(\"inertia_vs_K_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is an elbow at $K=5$, which means that less clusters than that would be bad, and more clusters would not help much and might cut clusters in half. So $K=5$ is a pretty good choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to look at the _silhouette score_, which is the mean _silhouette coefficient_ over all the instances. An instance's silhouette coefficient is equal to $(b - a)/\\max(a, b)$ where $a$ is the mean distance to the other instances in the same cluster (it is the _mean intra-cluster distance_), and $b$ is the _mean nearest-cluster distance_, that is the mean distance to the instances of the next closest cluster (defined as the one that minimizes $b$, excluding the instance's own cluster). The silhouette coefficient can vary between -1 and +1: a coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the silhouette score as a function of $K$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "# Get the silhouette score for the current model\n",
    "silhouette_score(X, kmeans.labels_)\n",
    "# Get the silhouette score for different models with different K\n",
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$K$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "save_fig(\"silhouette_score_vs_k_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using clustering for image segmentation\n",
    "Image segmentation is the task of partitioning an image into multiple segments. We are going to do color segmentation here. We will simply assign pixels to the same segment if they have a similar color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread\n",
    "image = imread(\"flower.jpg\")\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is represented as a 3D array. The first dimension’s size is the height; the second is the width; and the third is the number of color channels, in this case red, green, and blue (RGB). In other words, for each pixel there is a 3D vector containing the intensities of red, green, and blue, each between 0 and 255. <br>\n",
    "The following code reshapes the array to get a long list of RGB colors, then it clusters these colors using K-Means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image/255.0 # convert to 0...1 scale\n",
    "X = image.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 16)\n",
    "kmeans.fit(X)\n",
    "new_colors = kmeans.cluster_centers_[kmeans.predict(X)]\n",
    "new_colors = new_colors.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Mini-Batch K-Means to solve the same problem.\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.subplot(121)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(new_colors)\n",
    "plt.title(\"16-color image\")\n",
    "plt.axis('off')\n",
    "\n",
    "save_fig('image_segmentation_diagram', tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the program and complete the following exercises. (20 points)\n",
    "1.  For the \"lab06_data.txt\" KMeans example:\n",
    "    1. (2 points) check the value of \"y_kmeans\". Comparing to the model variables, What observation do you get?  \n",
    "    2. (3 points) Check which cluster the following instances belong to? (-1, 0.5), (6.5, 4.5), (2.2, 8.5) \n",
    "    3. (2 points) Print the centroids found by the user defined KMeans algorithm. Comparing to the sklearn KMeans method result, do they converge to the same centroids? \n",
    "2. (3 points) When Kmeans algorithm does not converge to the right solution, what do you think it converges to? Analyze the reasons. (What does it depend on?)\n",
    "3. (3 points) We cannot simply choose the value of K that minimizes the inertia, since it keeps getting lower as we increase K. Anylyze the reason.   \n",
    "4. For the image segmentation example,  \n",
    "    1. (2 points) Observe the data from the original picture and calulate possible color options in the original picture. \n",
    "    2. (1 points) What does the following statement do?<br>\n",
    "     ```python X = image.reshape(-1, 3) ```\n",
    "    3. (3 points) Use Mini-Batch K-Means to solve the same problem. Use \"%timeit\" function call to compare the time efficiency of the two algorithms.\n",
    "5. (1 points) Create a new Markdown field at the end of this file and put your answers in this field. Submit this file to the Blackboard. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
